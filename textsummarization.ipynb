{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyP3N2U4kTEUxE1mjM5UJcxT",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pritam187/TextSummarization1/blob/main/textsummarization.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MYQki45SNdkm",
        "outputId": "3a19064d-3580-473c-e48a-4209aa3ad445"
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: PyPDF2 in /usr/local/lib/python3.11/dist-packages (3.0.1)\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/pip/_internal/cli/base_command.py\", line 179, in exc_logging_wrapper\n",
            "    status = run_func(*args)\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/pip/_internal/cli/req_command.py\", line 67, in wrapper\n",
            "    return func(self, options, args)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/pip/_internal/commands/install.py\", line 447, in run\n",
            "    conflicts = self._determine_conflicts(to_install)\n",
            "                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/pip/_internal/commands/install.py\", line 578, in _determine_conflicts\n",
            "    return check_install_conflicts(to_install)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/pip/_internal/operations/check.py\", line 101, in check_install_conflicts\n",
            "    package_set, _ = create_package_set_from_installed()\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/pip/_internal/operations/check.py\", line 42, in create_package_set_from_installed\n",
            "    dependencies = list(dist.iter_dependencies())\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/pip/_internal/metadata/importlib/_dists.py\", line 225, in iter_dependencies\n",
            "    elif not extras and req.marker.evaluate({\"extra\": \"\"}):\n",
            "                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/pip/_vendor/packaging/markers.py\", line 325, in evaluate\n",
            "    return _evaluate_markers(self._markers, current_environment)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/pip/_vendor/packaging/markers.py\", line 224, in _evaluate_markers\n",
            "    lhs_value, rhs_value = _normalize(lhs_value, rhs_value, key=environment_key)\n",
            "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/pip/_vendor/packaging/markers.py\", line 198, in _normalize\n",
            "    return tuple(canonicalize_name(v) for v in values)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/pip/_vendor/packaging/markers.py\", line 198, in <genexpr>\n",
            "    return tuple(canonicalize_name(v) for v in values)\n",
            "                 ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/pip/_vendor/packaging/utils.py\", line 45, in canonicalize_name\n",
            "    def canonicalize_name(name: str, *, validate: bool = False) -> NormalizedName:\n",
            "    \n",
            "KeyboardInterrupt\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/bin/pip3\", line 10, in <module>\n",
            "    sys.exit(main())\n",
            "             ^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/pip/_internal/cli/main.py\", line 80, in main\n",
            "    return command.main(cmd_args)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/pip/_internal/cli/base_command.py\", line 100, in main\n",
            "    return self._main(args)\n",
            "           ^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/pip/_internal/cli/base_command.py\", line 232, in _main\n",
            "    return run(options, args)\n",
            "           ^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/pip/_internal/cli/base_command.py\", line 215, in exc_logging_wrapper\n",
            "    logger.critical(\"Operation cancelled by user\")\n",
            "  File \"/usr/lib/python3.11/logging/__init__.py\", line 1536, in critical\n",
            "    self._log(CRITICAL, msg, args, **kwargs)\n",
            "  File \"/usr/lib/python3.11/logging/__init__.py\", line 1634, in _log\n",
            "    self.handle(record)\n",
            "  File \"/usr/lib/python3.11/logging/__init__.py\", line 1644, in handle\n",
            "    self.callHandlers(record)\n",
            "  File \"/usr/lib/python3.11/logging/__init__.py\", line 1706, in callHandlers\n",
            "    hdlr.handle(record)\n",
            "  File \"/usr/lib/python3.11/logging/__init__.py\", line 978, in handle\n",
            "    self.emit(record)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/pip/_internal/utils/logging.py\", line 172, in emit\n",
            "    style = Style(color=\"red\")\n",
            "            ^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/pip/_vendor/rich/style.py\", line 146, in __init__\n",
            "    def _make_color(color: Union[Color, str]) -> Color:\n",
            "                           ~~~~~^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.11/typing.py\", line 376, in inner\n",
            "    return cached(*args, **kwds)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.11/typing.py\", line 502, in __getitem__\n",
            "    return self._getitem(self, parameters)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.11/typing.py\", line 715, in Union\n",
            "    parameters = tuple(_type_check(p, msg) for p in parameters)\n",
            "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.11/typing.py\", line 715, in <genexpr>\n",
            "    parameters = tuple(_type_check(p, msg) for p in parameters)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "KeyboardInterrupt\n",
            "^C\n"
          ]
        }
      ],
      "source": [
        "!pip install PyPDF2\n",
        "from IPython import get_ipython\n",
        "from IPython.display import display\n",
        "# %%\n",
        "#Step 1. Importing Libraries\n",
        "\n",
        "import sys\n",
        "import math\n",
        "import bs4 as bs\n",
        "import urllib.request\n",
        "import re\n",
        "import PyPDF2\n",
        "import nltk\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "import spacy\n",
        "\n",
        "\n",
        "#Execute this line if you are running this code for first time\n",
        "# Consider running this as a separate cell or checking if 'wordnet' is already downloaded\n",
        "# nltk.download('wordnet')\n",
        "\n",
        "#Initializing few variable\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "\n",
        "#Step 2. Define functions for Reading Input Text\n",
        "\n",
        "#Function to Read .txt File and return its Text\n",
        "def file_text(filepath):\n",
        "    with open(filepath) as f:\n",
        "        text = f.read().replace(\"\\n\", '')\n",
        "        return text\n",
        "\n",
        "\n",
        "#Function to Read PDF File and return its Text\n",
        "def pdfReader(pdf_path):\n",
        "\n",
        "    with open(pdf_path, 'rb') as pdfFileObject:\n",
        "        # PyPDF2.PdfFileReader is deprecated, use PyPDF2.PdfReader\n",
        "        pdfReader = PyPDF2.PdfReader(pdfFileObject)\n",
        "        # pdfReader.numPages is deprecated, use len(pdfReader.pages)\n",
        "        count = len(pdfReader.pages)\n",
        "        print(\"\\nTotal Pages in pdf = \", count)\n",
        "\n",
        "        c = 'Y'\n",
        "        start_page = 0\n",
        "        end_page = count-1\n",
        "        c = input(\"Do you want to read entire pdf ?[Y]/N  :  \")\n",
        "        if c.upper() == 'N' : # Using .upper() for case-insensitive comparison\n",
        "            start_page  = int(input(\"Enter start page number (Indexing start from 0) :  \"))\n",
        "            # Adjust end_page input prompt for clarity\n",
        "            end_page = int(input(f\"Enter end page number (Indexing start from 0, less than {count}) : \"))\n",
        "\n",
        "            if start_page <0 or start_page >= count:\n",
        "                print(\"\\nInvalid Start page given\")\n",
        "                sys.exit()\n",
        "\n",
        "            if end_page <0 or end_page >= count or end_page < start_page: # Added check for end_page < start_page\n",
        "                print(\"\\nInvalid End page given\")\n",
        "                sys.exit()\n",
        "\n",
        "        text = \"\" # Initialize text variable to accumulate text from pages\n",
        "        for i in range(start_page,end_page+1):\n",
        "            # pdfReader.getPage(i) is deprecated, use pdfReader.pages[i]\n",
        "            page = pdfReader.pages[i]\n",
        "            # extractText() is deprecated, use extract_text()\n",
        "            text += page.extract_text()\n",
        "\n",
        "        return text # Return the accumulated text\n",
        "\n",
        "#Function to Read wikipedia page url and return its Text\n",
        "def wiki_text(url):\n",
        "    scrap_data = urllib.request.urlopen(url)\n",
        "    article = scrap_data.read()\n",
        "    parsed_article = bs.BeautifulSoup(article,'lxml')\n",
        "\n",
        "    paragraphs = parsed_article.find_all('p')\n",
        "    article_text = \"\"\n",
        "\n",
        "    for p in paragraphs:\n",
        "        article_text += p.text\n",
        "\n",
        "    #Removing all unwanted characters\n",
        "    article_text = re.sub(r'\\[[0-9]*\\]', '', article_text)\n",
        "    return article_text\n",
        "\n",
        "\n",
        "#Step 3. Getting Text\n",
        "\n",
        "input_text_type = int(input(\"Select one way of inputting your text  \\\n",
        ": \\n1. Type your Text(or Copy-Paste)\\n2. Load from .txt file\\n3. Load from .pdf file\\n4. From Wikipedia Page URL\\n\\n\"))\n",
        "\n",
        "if input_text_type == 1:\n",
        "    text = input(u\"Enter your text : \\n\\n\")\n",
        "\n",
        "elif input_text_type == 2:\n",
        "    txt_path = input(\"Enter file path :  \")\n",
        "    text = file_text(txt_path)\n",
        "\n",
        "\n",
        "elif input_text_type == 3:\n",
        "    file_path = input(\"Enter file path :  \")\n",
        "    text = pdfReader(file_path)\n",
        "\n",
        "elif input_text_type == 4:\n",
        "    wiki_url = input(\"Enter Wikipedia URL to load Article : \")\n",
        "    text = wiki_text(wiki_url)\n",
        "\n",
        "else:\n",
        "    print(\"Sorry! Wrong Input, Try Again.\")\n",
        "    sys.exit() # Added sys.exit() to stop execution after wrong input\n",
        "\n",
        "\n",
        "#Step 4. Defining functions to create Tf-Idf Matrix\n",
        "\n",
        "\n",
        "#Function to calculate frequency of word in each sentence\n",
        "#INPUT -> List of all sentences from text as spacy.Doc object\n",
        "#OUTPUT -> freq_matrix (A dictionary with each sentence itself as key,\n",
        "# and a dictionary of words of that sentence with their frequency as value)\n",
        "\n",
        "def frequency_matrix(sentences):\n",
        "    freq_matrix = {}\n",
        "    stopWords = nlp.Defaults.stop_words\n",
        "\n",
        "    for sent in sentences:\n",
        "        freq_table = {} #dictionary with 'words' as key and their 'frequency' as value\n",
        "\n",
        "        #Getting all word from the sentence in lower case\n",
        "        words = [word.text.lower() for word in sent  if word.text.isalnum()]\n",
        "\n",
        "        for word in words:\n",
        "            word = lemmatizer.lemmatize(word)   #Lemmatize the word\n",
        "            if word not in stopWords:           #Reject stopwords\n",
        "                if word in freq_table:\n",
        "                    freq_table[word] += 1\n",
        "                else:\n",
        "                    freq_table[word] = 1\n",
        "\n",
        "        # Using the entire sentence text as key for better matching in create_summary\n",
        "        freq_matrix[sent.text] = freq_table\n",
        "\n",
        "    return freq_matrix\n",
        "\n",
        "\n",
        "#Function to calculate Term Frequency(TF) of each word\n",
        "#INPUT -> freq_matrix\n",
        "#OUTPUT -> tf_matrix (A dictionary with each sentence itself as key,\n",
        "# and a dictionary of words of that sentence with their Term-Frequency as value)\n",
        "\n",
        "#TF(t) = (Number of times term t appears in  document) / (Total number of terms in the document)\n",
        "def tf_matrix(freq_matrix):\n",
        "    tf_matrix = {}\n",
        "\n",
        "    for sent, freq_table in freq_matrix.items():\n",
        "        tf_table = {}  #dictionary with 'word' itself as a key and its TF as value\n",
        "\n",
        "        total_words_in_sentence = len(freq_table)\n",
        "        if total_words_in_sentence == 0: # Avoid division by zero for empty sentences\n",
        "            continue\n",
        "        for word, count in freq_table.items():\n",
        "            tf_table[word] = count / total_words_in_sentence\n",
        "\n",
        "        tf_matrix[sent] = tf_table\n",
        "\n",
        "    return tf_matrix\n",
        "\n",
        "\n",
        "#Function to find how many sentences contain a 'word'\n",
        "#INPUT -> freq_matrix\n",
        "#OUTPUT -> sent_per_words (Dictionary with each word itself as key and number of\n",
        "#sentences containing that word as value)\n",
        "\n",
        "def sentences_per_words(freq_matrix):\n",
        "    sent_per_words = {}\n",
        "\n",
        "    for sent, f_table in freq_matrix.items():\n",
        "        for word, count in f_table.items():\n",
        "            if word in sent_per_words:\n",
        "                sent_per_words[word] += 1\n",
        "            else:\n",
        "                sent_per_words[word] = 1\n",
        "\n",
        "    return sent_per_words\n",
        "\n",
        "\n",
        "#Function to calculate Inverse Document frequency(IDF) for each word\n",
        "#INPUT -> freq_matrix,sent_per_words, total_sentences\n",
        "#OUTPUT -> idf_matrix (A dictionary with each sentence itself as key,\n",
        "# and a dictionary of words of that sentence with their IDF as value)\n",
        "\n",
        "#IDF(t) = log_e(Total number of documents / Number of documents with term t in it)\n",
        "def idf_matrix(freq_matrix, sent_per_words, total_sentences):\n",
        "    idf_matrix = {}\n",
        "\n",
        "    for sent, f_table in freq_matrix.items():\n",
        "        idf_table = {}\n",
        "\n",
        "        for word in f_table.keys():\n",
        "            # Avoid division by zero if word is not in sent_per_words (shouldn't happen with current logic, but good practice)\n",
        "            if word in sent_per_words and sent_per_words[word] > 0:\n",
        "                idf_table[word] = math.log10(total_sentences / float(sent_per_words[word]))\n",
        "            else:\n",
        "                idf_table[word] = 0 # Assign 0 or handle as appropriate if word count is zero\n",
        "\n",
        "        idf_matrix[sent] = idf_table\n",
        "\n",
        "    return idf_matrix\n",
        "\n",
        "\n",
        "#Function to calculate Tf-Idf score of each word\n",
        "#INPUT -> tf_matrix, idf_matrix\n",
        "#OUTPUT - > tf_idf_matrix (A dictionary with each sentence itself as key,\n",
        "# and a dictionary of words of that sentence with their Tf-Idf as value)\n",
        "def tf_idf_matrix(tf_matrix, idf_matrix):\n",
        "    tf_idf_matrix = {}\n",
        "\n",
        "    # Use common keys between tf_matrix and idf_matrix to ensure alignment\n",
        "    for sent in tf_matrix.keys() & idf_matrix.keys():\n",
        "        tf_idf_table = {}\n",
        "        tf_table1 = tf_matrix[sent]\n",
        "        idf_table2 = idf_matrix[sent]\n",
        "\n",
        "        # Iterate through words present in both tables\n",
        "        for word in tf_table1.keys() & idf_table2.keys():\n",
        "            tf_idf_table[word] = float(tf_table1[word] * idf_table2[word])\n",
        "\n",
        "        tf_idf_matrix[sent] = tf_idf_table\n",
        "\n",
        "    return tf_idf_matrix\n",
        "\n",
        "\n",
        "#Function to rate every sentence with some score calculated on basis of Tf-Idf\n",
        "#INPUT -> tf_idf_matrix\n",
        "#OUTPUT - > sentenceScore (Dictionary with each sentence itself as key and its score\n",
        "# as value)\n",
        "def score_sentences(tf_idf_matrix):\n",
        "\n",
        "    sentenceScore = {}\n",
        "\n",
        "    for sent, f_table in tf_idf_matrix.items():\n",
        "        total_tfidf_score_per_sentence = 0\n",
        "\n",
        "        total_words_in_sentence = len(f_table)\n",
        "        if total_words_in_sentence == 0: # Avoid division by zero for empty sentences\n",
        "            sentenceScore[sent] = 0\n",
        "            continue\n",
        "        for word, tf_idf_score in f_table.items():\n",
        "            total_tfidf_score_per_sentence += tf_idf_score\n",
        "\n",
        "        sentenceScore[sent] = total_tfidf_score_per_sentence / total_words_in_sentence\n",
        "\n",
        "    return sentenceScore\n",
        "\n",
        "\n",
        "\n",
        "#Function Calculating average sentence score\n",
        "#INPUT -> sentence_score\n",
        "#OUTPUT -> average_sent_score(An average of the sentence_score)\n",
        "def average_score(sentence_score):\n",
        "\n",
        "    total_score = 0\n",
        "    # Check if sentence_score is empty to avoid division by zero\n",
        "    if not sentence_score:\n",
        "        return 0\n",
        "\n",
        "    for sent in sentence_score:\n",
        "        total_score += sentence_score[sent]\n",
        "\n",
        "    average_sent_score = (total_score / len(sentence_score))\n",
        "\n",
        "    return average_sent_score\n",
        "\n",
        "\n",
        "#Function to return summary of article\n",
        "#INPUT -> sentences(list of all sentences in article), sentence_score, threshold\n",
        "# (set to the average pf sentence_score)\n",
        "#OUTPUT -> summary (String text)\n",
        "def create_summary(sentences, sentence_score, threshold):\n",
        "    summary = ''\n",
        "\n",
        "    for sentence in sentences:\n",
        "        # Check if the sentence text is in sentence_score dictionary\n",
        "        if sentence.text in sentence_score and sentence_score[sentence.text] >= (threshold):\n",
        "            summary += \" \" + sentence.text\n",
        "\n",
        "    return summary\n",
        "\n",
        "\n",
        "#Step 5. Using all functions to generate summary\n",
        "\n",
        "# Check if text is empty after input selection\n",
        "if not text:\n",
        "    print(\"No text was loaded. Exiting.\")\n",
        "    sys.exit()\n",
        "\n",
        "\n",
        "#Counting number of words in original article\n",
        "# Consider using spacy tokens for more accurate word count (excluding punctuation etc.)\n",
        "original_words = text.split()\n",
        "original_words = [w for w in original_words if w.isalnum()] # This already filters out some non-alphanumeric\n",
        "num_words_in_original_text = len(original_words)\n",
        "\n",
        "\n",
        "#Converting received text into sapcy Doc object\n",
        "text_doc = nlp(text) # Renamed variable to avoid overwriting the original text string\n",
        "\n",
        "\n",
        "#Extracting all sentences from the text in a list\n",
        "sentences = list(text_doc.sents)\n",
        "total_sentences = len(sentences)\n",
        "\n",
        "# Handle case where there are no sentences\n",
        "if total_sentences == 0:\n",
        "    print(\"No sentences found in the text. Cannot generate summary.\")\n",
        "    sys.exit()\n",
        "\n",
        "\n",
        "#Generating Frequency Matrix\n",
        "freq_matrix = frequency_matrix(sentences)\n",
        "\n",
        "#Generating Term Frequency Matrix\n",
        "tf_matrix = tf_matrix(freq_matrix)\n",
        "\n",
        "#Getting number of sentences containing a particular word\n",
        "num_sent_per_words = sentences_per_words(freq_matrix)\n",
        "\n",
        "#Generating ID Frequency Matrix\n",
        "idf_matrix = idf_matrix(freq_matrix, num_sent_per_words, total_sentences)\n",
        "\n",
        "#Generating Tf-Idf Matrix\n",
        "tf_idf_matrix = tf_idf_matrix(tf_matrix, idf_matrix)\n",
        "\n",
        "\n",
        "#Generating Sentence score for each sentence\n",
        "sentence_scores = score_sentences(tf_idf_matrix)\n",
        "\n",
        "# Setting threshold to average value (You are free to play with ther values)\n",
        "threshold = average_score(sentence_scores)\n",
        "\n",
        "# Handle case where average score is 0 (e.g., very short input, only stopwords)\n",
        "if threshold == 0:\n",
        "    print(\"Average sentence score is zero. Cannot generate summary with a threshold.\")\n",
        "    summary = \"\" # Assign empty summary\n",
        "else:\n",
        "    #Getting summary\n",
        "    summary = create_summary(sentences, sentence_scores, 1.3 * threshold)\n",
        "\n",
        "print(\"\\n\\n\")\n",
        "print(\"*\"*20,\"Summary\",\"*\"*20)\n",
        "print(\"\\n\")\n",
        "print(summary)\n",
        "print(\"\\n\\n\")\n",
        "print(\"Total words in original article = \", num_words_in_original_text)\n",
        "print(\"Total words in summarized article = \", len(summary.split()))"
      ]
    }
  ]
}